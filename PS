Feature Selection :
Feature selection is the process of selecting a subset of relevant features (variables or attributes) from a larger set of available features. 
The goal is to identify the most informative and discriminative features that contribute the most to the predictive power of a machine learning model.
 By selecting the most relevant features, you can improve the model's performance by reducing overfitting, improving generalization, and increasing accuracy.
Removing irrelevant or redundant features simplifies the model, making it easier to understand and interpret. It can also lead to faster training and prediction times.
a.)Filter Methods: These techniques rank features based on statistical scores or correlation with the target variable, without involving the learning algorithm.
Examples include:
i.)Information Gain
ii.)Chi-Square Test
ii.)Correlation-based Feature Selection
b. Wrapper Methods: These methods evaluate subsets of features by training and evaluating the model using different feature combinations. Examples include:
Recursive Feature Elimination (RFE)
Genetic Algorithms
Forward/Backward Selection
c. Embedded Methods: These techniques perform feature selection as an integral part of the model training process. Examples include:
L1 Regularization (Lasso)
Tree-based Feature Importance (Random Forest, Gradient Boosting)
These are few methods for feature selection which I would describe each one by one based on the dataset I have considered
